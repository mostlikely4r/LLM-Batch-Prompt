# LLM-Batch-Prompt
 A utility program to batch prompt multiple models using koboldcpp

See also config.conf.

This program will start-up koboldcpp with a number of models and batch-prompt to get the different responses.
The responses are exported to a .json file to quicky see the difference between the models.