# The location of the koboldcpp Executable.
koboldPath = C:\kobolcpp\koboldcpp.exe

# The location where the models are located.
modelPath = C:\oobabooga_windows\text-generation-webui\models\

# The default start parameters used to start kobolcpp regardless of model.
startupParams = --usecublas 0 0 --threads 10 --highpriority --smartcontext --usemirostat 2 0.1 0

# The file containing one or multiple prompts seperated by the prompt seperator.
promptFile = prompts.txt

# The prompt seperator (at the start of a line) which seperate different prompts.
promptSeperator = <prompt_end>

# Generate url to be used to call Koboldcpp
promptUrl = http://127.0.0.1:5001/api/v1/generate

# Parameters to be used in generation
promptParams = max_length:180,max_context_length:2048,sampler_seed:1,top_p:0.9,typical:1,tfs=0.9,top_a=0,top_k=0,rep_pen=1.1,rep_pen_slope=0.9,rep_pen_range=1024,temperature=0.65,singleline=$f

# Show the kobopcpp output in the window.
showKoboldOutput = 0

# The output file where the replies for each prompt and model will be written.
outputFile = output.json
 
# The filename and specific parameters for each model to be tested.

#modelName1 = airoboros-33b-gpt4-1.4.1-PI-8192-ggmlv3.Q4_K_M.bin --gpulayers 20 --contextsize 8192 --linearrope
modelName2 = llama-2-7b.ggmlv3.q4_K_M.bin --gpulayers 43
modelName3 = Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_S.bin --gpulayers 43 
#modelName4 = chronos-wizardlm-uc-scot-st-13B.ggmlv3.q4_K_S.bin --gpulayers 43
#modelName5 = chronos-wizardlm-uc-scot-st-13B.ggmlv3.q4_K_S.bin --gpulayers 43
#modelName6 = gpt4-x-alpaca-30b-ggml-q4_1.bin --gpulayers 25
